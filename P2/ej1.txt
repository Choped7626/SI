----------------------------------------------------------------1 capa oculta--------------------------------------------------------------------

precisionEntrenamiento = [];
precisionValidacion = [];
precisionTest = [];
for i = 1:10,
rna = patternnet([i]);
rna.trainParam.showWindow = false;
[rna tr] = train(rna, entradas, salidas);
outputs = sim(rna, entradas);
precisionEntrenamiento(end+1) = 1-confusion(salidas(:,tr.trainInd),outputs(:,tr.trainInd));
precisionValidacion(end+1) = 1-confusion(salidas(:,tr.valInd),outputs(:,tr.valInd));
precisionTest(end+1) = 1-confusion(salidas(:,tr.testInd),outputs(:,tr.testInd));
end;

_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-

>>¿Cuándo y por qué se para el entrenamiento?

el entrenamiento se detiene siempre al alcanzar la meta de 6 resultados iguales (validations check), esto normalmente suele ocurrir antes de llegar siquiera a las 100 iteraciones

_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-

>>Una vez que el entrenamiento se para, ¿cuáles son los resultados que se obtienen? ¿A qué ciclo corresponden? ¿Por qué son los de ese ciclo y no los de otro?

Epoch	0	37	1000
Elapsed Time	-	00:00:00	-
Performance	1.46	0.0889	0
Gradient	7.96	0.0137	1e-06
Validation Checks	0	6	6

Best Validation Performance is 0.10052 at epoch 31

Los resultados obtenidos pertenecen a ese ciclo y no a otro, por ejemplo el ultimo, porque los resultados se escojen en función de el ciclo con mejor conjunto de validación
(menor error de validación)

_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-

precisionValidacion	[0.9511,0.9638,0.9648,0.9697,0.9687,0.9629,0.9658,0.9599,0.9785,0.9658]	double
precisionTest	[0.9589,0.9687,0.9658,0.9638,0.9687,0.9609,0.9638,0.9707,0.9658,0.9726]	double
precisionEntrenamiento	[0.9717,0.9707,0.9688,0.9705,0.9690,0.9738,0.9709,0.9709,0.9658,0.9698]	double

-----------------------------------------------------------------2 capas ocultas-----------------------------------------------------------------

precisionEntrenamiento = [];
precisionValidacion = [];
precisionTest = [];
for i = 1:10,
for j = 1:20,
rna = patternnet([i j]);
[rna tr] = train(rna, entradas, salidas);
outputs = sim(rna, entradas);
precisionEntrenamiento(end+1) = 1-confusion(salidas(:,tr.trainInd),outputs(:,tr.trainInd));
precisionValidacion(end+1) = 1-confusion(salidas(:,tr.valInd),outputs(:,tr.valInd));
precisionTest(end+1) = 1-confusion(salidas(:,tr.testInd),outputs(:,tr.testInd));
end;
end;

_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-

>>¿Cuándo y por qué se para el entrenamiento?

El entrenamiento se detiene siempre al alcanzar la meta de 6 resultados iguales (validations check), esto normalmente suele ocurrir antes de llegar siquiera a las 100 iteraciones

_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-

>>Una vez que el entrenamiento se para, ¿cuáles son los resultados que se obtienen? ¿A qué ciclo corresponden? ¿Por qué son los de ese ciclo y no los de otro?

Epoch	0	31	1000
Elapsed Time	-	00:00:00	-
Performance	0.478	0.0861	0
Gradient	3.16	0.0144	1e-06
Validation Checks	0	6	6

Best Validation Performance is 0.078329 at epoch 25

Los resultados obtenidos pertenecen a ese ciclo y no a otro, por ejemplo el ultimo, porque los resultados se escojen en función de el ciclo con mejor conjunto de validación
(menor error de validación)

(son los datos de la iteracion 200, con ([10 20]).)
_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-

Precesion Entrenamiento = 

0.9635	0.9694	0.9679	0.9671	0.9673	0.9673	0.9702	0.9690	0.9705	0.9698	0.9698	0.9692	0.9658	0.9688	0.9690	0.9673	0.9677	0.9692	0.9677	0.9686	0.9705	0.9686	0.9671	0.9690	0.9673	0.9700	0.9726	0.9679	0.9694	0.9675	0.9682	0.9675	0.9667	0.9707	0.9665	0.9696	0.9702	0.9688	0.9673	0.9677	0.9709	0.9661	0.9671	0.9707	0.9679	0.9675	0.9698	0.9682	0.9667	0.9686	0.9658	0.9723	0.9671	0.9679	0.9684	0.9694	0.9669	0.9675	0.9684	0.9692	0.9696	0.9684	0.9667	0.9705	0.9671	0.9677	0.9682	0.9702	0.9711	0.9711	0.9709	0.9650	0.9677	0.9705	0.9692	0.9711	0.9696	0.9679	0.9702	0.9690	0.9694	0.9661	0.9705	0.9661	0.9690	0.9696	0.9677	0.9696	0.9663	0.9665	0.9669	0.9671	0.9677	0.9711	0.9705	0.9673	0.9709	0.9677	0.9682	0.9692	0.9694	0.9684	0.9675	0.9686	0.9688	0.9694	0.9690	0.9665	0.9667	0.9707	0.9688	0.9702	0.9675	0.9709	0.9684	0.9717	0.9675	0.9696	0.9698	0.9688	0.9673	0.9677	0.9742	0.9705	0.9696	0.9696	0.9702	0.9688	0.9696	0.9713	0.9682	0.9679	0.9679	0.9700	0.9682	0.9669	0.9694	0.9671	0.9726	0.9715	0.9661	0.9682	0.9702	0.9677	0.9661	0.9692	0.9702	0.9690	0.9686	0.9728	0.9705	0.9719	0.9661	0.9694	0.9694	0.9690	0.9682	0.9679	0.9715	0.9696	0.9715	0.9690	0.9698	0.9686	0.9705	0.9702	0.9669	0.9694	0.9690	0.9694	0.9709	0.9673	0.9690	0.9682	0.9696	0.9715	0.9721	0.9669	0.9705	0.9682	0.9694	0.9677	0.9702	0.9661	0.9679	0.9658	0.9694	0.9690	0.9709	0.9705	0.9694	0.9682	0.9702	0.9713	0.9686	0.9654	0.9690	0.9705	0.9705	0.9696

Precision Test = 

0.9726	0.9726	0.9677	0.9726	0.9736	0.9648	0.9677	0.9687	0.9609	0.9677	0.9668	0.9687	0.9687	0.9648	0.9619	0.9726	0.9638	0.9609	0.9629	0.9717	0.9707	0.9697	0.9726	0.9687	0.9697	0.9756	0.9560	0.9697	0.9658	0.9717	0.9775	0.9697	0.9746	0.9717	0.9638	0.9677	0.9717	0.9580	0.9756	0.9658	0.9648	0.9746	0.9668	0.9599	0.9785	0.9677	0.9638	0.9677	0.9677	0.9736	0.9648	0.9638	0.9726	0.9697	0.9736	0.9717	0.9756	0.9717	0.9736	0.9668	0.9707	0.9687	0.9746	0.9629	0.9707	0.9775	0.9746	0.9677	0.9707	0.9638	0.9687	0.9756	0.9677	0.9687	0.9677	0.9629	0.9717	0.9658	0.9648	0.9609	0.9648	0.9697	0.9697	0.9697	0.9717	0.9677	0.9677	0.9658	0.9668	0.9765	0.9736	0.9658	0.9668	0.9658	0.9668	0.9697	0.9599	0.9717	0.9658	0.9648	0.9629	0.9804	0.9707	0.9717	0.9687	0.9619	0.9668	0.9746	0.9736	0.9677	0.9668	0.9619	0.9707	0.9697	0.9717	0.9677	0.9677	0.9756	0.9609	0.9668	0.9707	0.9629	0.9570	0.9677	0.9648	0.9609	0.9697	0.9717	0.9717	0.9668	0.9765	0.9717	0.9746	0.9677	0.9726	0.9765	0.9736	0.9717	0.9668	0.9589	0.9580	0.9687	0.9687	0.9707	0.9648	0.9736	0.9668	0.9707	0.9629	0.9501	0.9619	0.9560	0.9746	0.9619	0.9697	0.9697	0.9726	0.9717	0.9589	0.9668	0.9570	0.9629	0.9677	0.9736	0.9648	0.9648	0.9687	0.9658	0.9658	0.9756	0.9570	0.9765	0.9726	0.9746	0.9668	0.9726	0.9638	0.9697	0.9736	0.9658	0.9717	0.9599	0.9658	0.9814	0.9677	0.9697	0.9726	0.9629	0.9658	0.9707	0.9697	0.9717	0.9658	0.9736	0.9736	0.9804	0.9658	0.9638	0.9560	0.9599

Precisión Validación = 

0.9824	0.9629	0.9668	0.9658	0.9648	0.9726	0.9609	0.9619	0.9687	0.9697	0.9648	0.9638	0.9736	0.9736	0.9717	0.9668	0.9599	0.9677	0.9677	0.9726	0.9638	0.9668	0.9658	0.9697	0.9756	0.9560	0.9697	0.9697	0.9619	0.9677	0.9609	0.9668	0.9658	0.9609	0.9775	0.9697	0.9619	0.9648	0.9736	0.9726	0.9668	0.9687	0.9746	0.9736	0.9746	0.9687	0.9629	0.9726	0.9726	0.9697	0.9795	0.9619	0.9638	0.9648	0.9619	0.9658	0.9697	0.9648	0.9697	0.9726	0.9648	0.9707	0.9707	0.9658	0.9687	0.9697	0.9589	0.9638	0.9570	0.9638	0.9668	0.9726	0.9726	0.9570	0.9648	0.9619	0.9609	0.9668	0.9687	0.9736	0.9707	0.9736	0.9619	0.9736	0.9648	0.9619	0.9677	0.9609	0.9726	0.9726	0.9717	0.9726	0.9717	0.9648	0.9658	0.9736	0.9599	0.9707	0.9775	0.9697	0.9785	0.9677	0.9717	0.9599	0.9697	0.9677	0.9697	0.9765	0.9668	0.9541	0.9717	0.9589	0.9736	0.9619	0.9629	0.9629	0.9765	0.9609	0.9658	0.9638	0.9697	0.9707	0.9609	0.9668	0.9746	0.9736	0.9648	0.9697	0.9648	0.9638	0.9658	0.9638	0.9619	0.9726	0.9707	0.9726	0.9589	0.9707	0.9589	0.9668	0.9619	0.9697	0.9658	0.9648	0.9785	0.9619	0.9668	0.9736	0.9756	0.9717	0.9658	0.9648	0.9687	0.9717	0.9658	0.9697	0.9697	0.9697	0.9756	0.9648	0.9677	0.9697	0.9648	0.9697	0.9609	0.9687	0.9707	0.9658	0.9638	0.9677	0.9736	0.9707	0.9638	0.9658	0.9668	0.9648	0.9629	0.9746	0.9609	0.9726	0.9638	0.9756	0.9658	0.9648	0.9629	0.9814	0.9677	0.9687	0.9629	0.9629	0.9609	0.9658	0.9619	0.9531	0.9677	0.9717	0.9726	0.9736	0.9756	0.9717

